# An谩lisis Meteorol贸gico con AWS y Machine Learning

##  Objetivo del Proyecto
El prop贸sito de este repositorio es mostrar la manera en que se obtuvieron y analizaron los datos meteorol贸gicos (para este caso, se eligi贸 la Avenida Paseo de la Reforma en la Ciudad de M茅xico) utilizando la API de **Open-Meteor**, aplicando un proceso ETL mediante Python desde Amazon Web Services (AWS) y finalmente desarrollando un modelo de aprendizaje autom谩tico para evaluar patrones clim谩ticos.

En s铆, espero que esta documentaci贸n pueda ser una gu铆a en el paso a paso para quienes deseen replicarlo o tomarlo como base para otro tipo de proyectos.

## 1. Obtenci贸n de los Datos
Desde **Google Colab** se utiliz贸 la API de Open-Meteor para extraer informaci贸n meteorol贸gica diaria. Los par谩metros de inter茅s fueron los siguientes en un per铆odo de 2020 a 2024:

- **Temperatura m谩xima** (`temperature_2m_max`)
- **Temperatura m铆nima** (`temperature_2m_min`)
- **Temperatura media** (`temperature_2m_mean`)
- **Precipitaci贸n total** (`rain_sum`)
- **Horas de precipitaci贸n** (`precipitation_hours`)
- **Velocidad m谩xima del viento a 10m** (`wind_speed_10m_max`)
- **Radiaci贸n solar acumulada** (`shortwave_radiation_sum`)

Debido a que estos datos ser谩n usados desde AWS, dentro del notebook se realiza la conversi贸n del DataFrame a CSV para poder subirlo al bucket. El notebook se encuentra dentro de este repositorio bajo el nombre: [fetch_data_&_upload_to_S3.ipynb](https://github.com/kahiji052/Data-Engineer-Academy-Xideral-2025/blob/main/4th%20week/meteorological-data-analysis/fetch_data_%26_upload_to_S3.ipynb)

 **NOTA:** *en este caso no se instal贸 AWS CLI, lo considero opcional debido a que mediante las variables de entorno (Secrets) defin铆 las llaves de acceso al igual que el nombre del bucket.*

En la siguiente captura de pantalla se visualiza el archivo cargado al bucket definido desde el notebook.
![image](https://github.com/user-attachments/assets/4113cd0d-2a63-4d72-84ee-178ad6d365a9)


## 2. Proceso ETL en AWS
### Creaci贸n de Funci贸n Lambda
Antes de este paso y para evitar confusiones, es importante crear un rol para la funci贸n con los permisos para S3 y CloudWatch. El nombre en mi caso fue `lambda_weather_data`.
```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "s3:GetObject",
        "s3:PutObject"
      ],
      "Resource": [
        "arn:aws:s3:::<bucket-name>/*"
      ]
    },
    {
      "Effect": "Allow",
      "Action": [
        "logs:CreateLogGroup",
        "logs:CreateLogStream",
        "logs:PutLogEvents"
      ],
      "Resource": "arn:aws:logs:*:*:*"
    }
  ]
}
```
En la creaci贸n de la funci贸n s贸lo asign茅 el nombre y escog铆 la versi贸n m谩s reciente de Python (3.13), el resto se qued贸 por defecto.
![image](https://github.com/user-attachments/assets/3b79c572-2820-41a1-a91c-0f2d4362937b)

Una vez creada, tambi茅n es importante agregar un Layer. Para este caso, s贸lo fue necesario pandas y la configuraci贸n fue la siguiente.
![image](https://github.com/user-attachments/assets/af1fab6d-e0f7-4211-81f9-062973ff19bc)

Mi inter茅s es automatizar lo m谩s que se pueda, idealmente todo. Con el notebook anterior, el trigger para la funci贸n Lambda ser谩 un evento S3, espec铆ficamente el tipo de evento que se activa cuando un archivo con prefijo `weather_` es subido (PUT) al bucket. Descarga el archivo, lo limpia eliminando las filas duplicadas y valores nulos, luego sube el archivo procesado (con el prefijo processed_) nuevamente al bucket. De manera que para este punto ya se tiene el csv original y el procesado.

El c贸digo de la funci贸n en lambda tambi茅n se encuentra dentro del repositorio llamado [lambda_function.py](https://github.com/kahiji052/Data-Engineer-Academy-Xideral-2025/blob/main/4th%20week/meteorological-data-analysis/lamda_function.py)

![image](https://github.com/user-attachments/assets/053c1513-dafb-48ff-974e-a419edeb2069)

### Creaci贸n de Crawler para catalogar los datos
El Crawler detecta en autom谩tico el esquema de los datos meteorol贸gicos (al momento de su creaci贸n, es importante crear una base de datos nueva para que los esquemas sean cargados ah铆). En este caso, se cre贸 una tabla y adjunto el esquema.
![image](https://github.com/user-attachments/assets/9487a482-af2d-4ba7-9e50-729af49c1785)
![image](https://github.com/user-attachments/assets/dda254f5-de76-481b-b5fc-90ac33eb8abc)


### Glue Job para transformar los datos (Visual ETL)
AWS Glue Job permite realizar transformaciones en los datos a trav茅s de una interfaz visual. El job qued贸 de la siguiente manera, a continuaci贸n explico cada nodo:

![image](https://github.com/user-attachments/assets/46764ad1-f522-448e-8b5a-a21aafb7f316)

- **Data Source:** se escogi贸 el archivo procesado por la funci贸n lambda.
- **Transform - Change Schema:** se modific贸 la columna fecha para que tenga el tipo `DATE`. El resto de las columnas, al tener un valor decimal, se convirtieron a `FLOAT`.
- **Tranform - Identifier:** se gener贸 un campo id 煤nico para cada registro.
- **Data Target:** una vez que los datos se han transformado de la manera deseada, se suben al bucket. \

El archivo con los datos transformados tendr谩 un nombre predefinido por AWS como se ve aqu铆:
![image](https://github.com/user-attachments/assets/6879ed00-fad7-4c7c-bb12-6694fec3e2f0)

Y el archivo csv se encuentra aqu铆: [transformed_weather_data.csv](https://github.com/kahiji052/Data-Engineer-Academy-Xideral-2025/tree/main/4th%20week/meteorological-data-analysis/transformed_weather_data.csv)





