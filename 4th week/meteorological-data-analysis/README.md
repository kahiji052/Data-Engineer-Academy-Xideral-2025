# An谩lisis Meteorol贸gico con AWS y Machine Learning

##  Objetivo del Proyecto
El prop贸sito de este repositorio es mostrar la manera en que se obtuvieron y analizaron los datos meteorol贸gicos (para este caso, se eligi贸 la Avenida Paseo de la Reforma en la Ciudad de M茅xico) utilizando la API de **Open-Meteor**, aplicando un proceso ETL mediante Python desde Amazon Web Services (AWS) y finalmente desarrollando un modelo de aprendizaje autom谩tico para evaluar patrones clim谩ticos.

En s铆, espero que esta documentaci贸n pueda ser una gu铆a en el paso a paso para quienes deseen replicarlo o tomarlo como base para otro tipo de proyectos.

## 1. Obtenci贸n de los Datos
Desde **Google Colab** se utiliz贸 la API de Open-Meteor para extraer informaci贸n meteorol贸gica diaria. Los par谩metros de inter茅s fueron los siguientes en un per铆odo de 2020 a 2024:

- **Temperatura m谩xima** (`temperature_2m_max`)
- **Temperatura m铆nima** (`temperature_2m_min`)
- **Temperatura media** (`temperature_2m_mean`)
- **Precipitaci贸n total** (`rain_sum`)
- **Horas de precipitaci贸n** (`precipitation_hours`)
- **Velocidad m谩xima del viento a 10m** (`wind_speed_10m_max`)
- **Radiaci贸n solar acumulada** (`shortwave_radiation_sum`)

Debido a que estos datos ser谩n usados desde AWS, dentro del notebook se realiza la conversi贸n del DataFrame a CSV para poder subirlo al bucket. El notebook se encuentra dentro de este repositorio bajo el nombre: [fetch_data_&_upload_to_S3.ipynb](https://github.com/kahiji052/Data-Engineer-Academy-Xideral-2025/blob/main/4th%20week/meteorological-data-analysis/fetch_data_%26_upload_to_S3.ipynb)

 **NOTA:** *en este caso no se instal贸 AWS CLI, lo considero opcional debido a que mediante las variables de entorno (Secrets) defin铆 las llaves de acceso al igual que el nombre del bucket.*

En la siguiente captura de pantalla se visualiza el archivo cargado al bucket definido desde el notebook.
![image](https://github.com/user-attachments/assets/4113cd0d-2a63-4d72-84ee-178ad6d365a9)


## 2. Proceso ETL en AWS
### Creaci贸n de Funci贸n Lambda
Antes de este paso y para evitar confusiones, es importante crear un rol para la funci贸n con los permisos para S3 y CloudWatch. El nombre en mi caso fue `lambda_weather_data`.
```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "s3:GetObject",
        "s3:PutObject"
      ],
      "Resource": [
        "arn:aws:s3:::<bucket-name>/*"
      ]
    },
    {
      "Effect": "Allow",
      "Action": [
        "logs:CreateLogGroup",
        "logs:CreateLogStream",
        "logs:PutLogEvents"
      ],
      "Resource": "arn:aws:logs:*:*:*"
    }
  ]
}
```
En la creaci贸n de la funci贸n s贸lo asign茅 el nombre y escog铆 la versi贸n m谩s reciente de Python (3.13), el resto se qued贸 por defecto.
![image](https://github.com/user-attachments/assets/3b79c572-2820-41a1-a91c-0f2d4362937b)

Una vez creada, tambi茅n es importante agregar un Layer. Para este caso, s贸lo fue necesario pandas y la configuraci贸n fue la siguiente.
![image](https://github.com/user-attachments/assets/af1fab6d-e0f7-4211-81f9-062973ff19bc)

Mi inter茅s es automatizar lo m谩s que se pueda, idealmente todo. Con el notebook anterior, el trigger para la funci贸n Lambda ser谩 un evento S3, espec铆ficamente el tipo de evento que se activa cuando un archivo con prefijo `weather_` es subido (PUT) al bucket. Descarga el archivo, lo limpia eliminando las filas duplicadas y valores nulos, luego sube el archivo procesado (con el prefijo processed_) nuevamente al bucket. De manera que para este punto ya se tiene el csv original y el procesado.

El c贸digo de la funci贸n en lambda tambi茅n se encuentra dentro del repositorio llamado [lambda_function.py](https://github.com/kahiji052/Data-Engineer-Academy-Xideral-2025/blob/main/4th%20week/meteorological-data-analysis/lamda_function.py)

![image](https://github.com/user-attachments/assets/053c1513-dafb-48ff-974e-a419edeb2069)

### Creaci贸n de Crawler para catalogar los datos
El Crawler detecta en autom谩tico el esquema de los datos meteorol贸gicos (al momento de su creaci贸n, es importante crear una base de datos nueva para que los esquemas sean cargados ah铆). En este caso, se cre贸 una tabla y adjunto el esquema.
![image](https://github.com/user-attachments/assets/9487a482-af2d-4ba7-9e50-729af49c1785)
![image](https://github.com/user-attachments/assets/dda254f5-de76-481b-b5fc-90ac33eb8abc)


### Glue Job para transformar los datos (Visual ETL)
AWS Glue Job permite realizar transformaciones en los datos a trav茅s de una interfaz visual. El job qued贸 de la siguiente manera, a continuaci贸n explico cada nodo:

![image](https://github.com/user-attachments/assets/46764ad1-f522-448e-8b5a-a21aafb7f316)

- **Data Source:** se escogi贸 el archivo procesado por la funci贸n lambda.
- **Transform - Change Schema:** se modific贸 la columna fecha para que tenga el tipo `DATE`. El resto de las columnas, al tener un valor decimal, se convirtieron a `FLOAT`.
- **Tranform - Identifier:** se gener贸 un campo id 煤nico para cada registro.
- **Data Target:** una vez que los datos se han transformado de la manera deseada, se suben al bucket.

El archivo con los datos transformados tendr谩 un nombre predefinido por AWS como se ve aqu铆:
![image](https://github.com/user-attachments/assets/6879ed00-fad7-4c7c-bb12-6694fec3e2f0)

Y el archivo csv se encuentra aqu铆: [transformed_weather_data.csv](https://github.com/kahiji052/Data-Engineer-Academy-Xideral-2025/tree/main/4th%20week/meteorological-data-analysis/transformed_weather_data.csv)


## 3. An谩lisis Exploratorio de Datos (EDA)
Para todas las variables se implement贸 la transformaci贸n de datos de manera binaria y en multiclase, a pesar de que se eligi贸 `Radiation_Class` como la variable objetivo para el modelo de Machine Learning, se realizaron las transformaciones en todas para facilitar el entrenamiento del modelo mediante categor铆as.

- **Lluvia** (1- lluvia, 0 - no lluvia)
- **Radiaci贸n** (2 - alta radiaci贸n, 1 - radiaci贸n moderada, 0 - baja radiaci贸n)
- **Viento** (2 - vientos fuertes, 1 - viento con r谩fagas leves, 0 - sin viento)
- **Temperatura** (2 - Alta, 1 - Moderada, 0 - Baja)

Para realizar la exploraci贸n se implement贸:
- **Histograma:** para ver la distribuci贸n de las variables.
- **Gr谩ficos de Caja:** para identifican valores at铆picos (y as铆 evitar distorsiones en el modelo).
- **Matriz de correlaci贸n:** verificar las variables que est谩n relacionadas entre s铆.

Adicional, se hizo la gr谩fica para ver la temperatura m谩s dominante entre el 2020 a 2024. Para el caso de Reforma fue moderada como se ve en la siguiente imagen.

![image](https://github.com/user-attachments/assets/a867ac3e-0585-4b1d-99a0-6ac83e6380c2)


## 4. Preprocesamiento de Datos
Se aplicaron t茅cnicas de escalado para normalizar las variables num茅ricas, lo que mejora el rendimiento de los modelos de Machine Learning. En este caso, se utilizaron los siguientes escaladores:
- MinMaxScaler: Escala las variables a un rango de 0 a 1.
- StandardScaler: Escala las variables para que tengan una media de 0 y una desviaci贸n est谩ndar de 1.

## 5. Implementaci贸n de Modelo de Aprendizaje Autom谩tico
Se implement贸 el modelo `Random Forest` para la predicci贸n de la variable objetivo `Radiation_Class`, fue entrenado utilizando el conjunto de datos preprocesado y se utilizaron estas m茅tricas :
- Precisi贸n: fracci贸n de predicciones correctas sobre el total de predicciones.
- Exactitud: proporci贸n de predicciones positivas correctas sobre todas las predicciones positivas.
- Sensibilidad: capacidad del modelo para identificar correctamente los casos positivos (en este caso, las clases de radiaci贸n alta).


![image](https://github.com/user-attachments/assets/9bb260d3-922e-4bae-8b66-c5d52960bbdf)


## 6. Conclusiones
Principalmente, con este proyecto consolid茅 y apliqu茅 los conocimientos adquiridos en el transcurso de la academia. La implementaci贸n del pipeline ETL utilizando AWS permiti贸 automatizar el proceso de la adquisici贸n y limpieza de los datos proporcionados de la API. Y queda la posibilidad de implementar otro tipo de modelos, o en todo caso, ir probando diferentes variables objetivo; dado que para este caso, Random Forest permiti贸 evaluar la precisi贸n del modelo de predicci贸n con una presici贸n del 79%, lo cual no es tan alto pero quiz谩 pudo influir que en el caso de lluvias acumuladas la mayor铆a fue 0.
